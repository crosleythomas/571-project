<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>571 project | Crosley, Singh</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Action-Conditional Video Prediction</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/crosleythomas/571-project" class="btn">View on GitHub</a>
      <a href="https://github.com/crosleythomas/571-project/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/crosleythomas/571-project/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h3>
<a id="welcome" class="anchor" href="#welcome" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Welcome.</h3>

<p>This is the project website for Thomas Crosley and Karanbir Singh's UW CSE 571 class project.  On this page we will share updates on our progress. To view our open-source code you can visit the <a href='https://github.com/crosleythomas/571-project'>repo here</a>.</p>

<h3>
<a id="project" class="anchor" href="#project" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Project Summary</h3>
<p>
We are working on action-conditional video prediction as described in the 2015 paper by Oh, et al. <a href='https://arxiv.org/pdf/1507.08750v2.pdf'>Action-Conditional Video Prediction using Deep Networks in Atari Games</a>.  The goal for the project is to implement the baseline algorithm on simple datasets, and then scale up the complexity of the data and make adjustments to the algorithm accordingly.
</p>

<h2>
<a id="artifacts" class="anchor" href="#artifacts" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Artifacts</h3>

<p>
<a href="https://docs.google.com/presentation/d/1le7d_BaiItcNxzZWZnSHBSrobMlRR6UEj9CKUH69W48/pub?start=false&loop=false&delayms=3000">Here</a> is a link to our presentation.
</p>
<p>
<a href="https://docs.google.com/document/d/18ao1nZunO8xzh5dUyNDuN2fET9DzzyprHpXJCAIHT6M/pub">Here</a> is our final paper.
</p>
      
<h2>
<a id="milestones" class="anchor" href="#milestones" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Milestones</h3>
<p>
Below, we have set goals for the project so we can track progress through the end of the quarter.
</p>

<h3>
<a id="milestone-1" class="anchor" href="#milestone-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Milestone 1 (Midpoint - November 21st)</h3>
<ul>
<li>
Collect video datasets that include frame transitions that are stochastic given only the previous frame but deterministic given the frame and the action taken by the agent.
</li>
<li>
Set up development tools for training deep neural networks.  We will be using the <a href='https://github.com/fchollet/keras'Keras</a> framework with <a href='https://www.tensorflow.org/'>Tensorflow</a> as a back-end for development of the neural networks.
</li>
<li>
Implement the feed-forward version of the algorithm described in the paper in Keras.
</li>
<li>
Train the network on any basic datasets we have acquired.
</li>
<li>
Artifact: Datasets that are deterministic given an action.  An implementation of the feed-forward system described in the paper that runs on basic datasets.
</li>
<li>
<b>Results: (November 20th)</b>
<ul>
  <li>Our presentation is <a href="https://docs.google.com/presentation/d/1xfsEDvupzKLWcwzpQGDVae32iMeAl6tH6CXc_clFiK8/edit?usp=sharing">here.</a></li>
  <li>We generate a Pacman-style dataset, where the next frame is random given the current frame, but deterministic given a [left, right, up, down, stay] action.</li>
  <li>We train two feed-forward networks - one with actions and one without.</li>
  <li>We evaluate our model and calculate mean-squared loss.</li>
  <li>We write visualization code for predicted vs. true frames.</li>
</ul>
</li>
</ul>


<h3>
<a id="milestone-2" class="anchor" href="#milestone-2" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Milestone 2 (November 28th)</h3>
<ul>
<li>
Create a new dataset where the camera moves with the agent but is still a top-down view; the agent will be in the same place every time but the environment landmarks will move.
</li>
<li>
Concatenate the predictions as the agent moves to make a psuedo-map of the environment.
</li>
<li>
Adjust the architecture of the network/algorithm to address issues that arise for this new dataset and task.
</li>
<li>
<b>Artifact: New datasets with the agent centered in all frames.  A map of the environment from consecutive predictions.</b>
</li>
  <li>
<b>Results: (November 28th)</b>
<ul>
  <li>We generated datasets with smaller windows as the agent moved through a larger map, but preliminary results were tough to interpret. We changed directions.</li>
  <li>Code written to extend dataset-type - can generate grids of arbitrary size / walls / dots, although these inputs are much tougher to train on (size, etc))</li>
  <li>Progress made on informed exploration - agent uses the network to inform movement towards states different from its history.</li>
</ul>
</li>
</ul>

<h3>
<a id="milestone-3" class="anchor" href="#milestone-3" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Milestone 3 (Final - December 12th)</h3>
<ul>
<li>
Create even more complex datasets including those with noisy motion models, noisy observations models, and videos that are stochastic conditioned on both the previous frame(s) and action.
</li>
<li>
Try adding to the architecture to address problems that arose with the more complex datasets.  For example, try stochastic networks, sampling predictions, recurrent networks, etc as time allows.
</li>
<li>
<b>Artifact: an implementation of one more complex architecture - a recurrent network, a stochastic network, etc.  The advanced approach we choose to implement will be based on the short-comings we see from training on the more complex datasets.</b>
</li>
<li>
<b>Results: (December 12th)</b>
<ul>
  <li>We present an informed exploration strategy using the network to move towards new states.</li>
  <li>We simulate a noisy sensor model and evaluate our network on these frames. We create a recurrent model that performs better at this task.</li>
  <li>We examine the effect of the hidden encoded vector size on training loss.</li>
  <li>We investigate action-transformations, particularly when the input action is all 1s to view the state of the encoded vector.</li>
  <li>We present findings in class and in our paper. (Links at top of site).</li>
</ul>
</li> 
</ul>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Authors and Contributors</h3>

<p>This is the project website for <a href="https://github.com/crosleythomas" class="user-mention">@crosleythomas</a> and <a href="https://github.com/karanbirsingh" class="user-mention">@karanbirsingh</a>.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/crosleythomas/571-project">571-project</a> is maintained by <a href="https://github.com/crosleythomas">crosleythomas</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
